{"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9251615,"sourceType":"datasetVersion","datasetId":5597172},{"sourceId":101778,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":85330,"modelId":109548},{"sourceId":101779,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":85331,"modelId":109549},{"sourceId":101781,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":85333,"modelId":109551}],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/mdrejwanurrahman/notebook8640634a91?scriptVersionId=194607801\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.interpolate import interp1d\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport tensorflow as tf\nimport tensorflow_probability as tfp\nimport joblib","metadata":{"id":"8yjJr-iGvgdn","execution":{"iopub.status.busy":"2024-08-29T20:07:25.043361Z","iopub.execute_input":"2024-08-29T20:07:25.043761Z","iopub.status.idle":"2024-08-29T20:07:44.497446Z","shell.execute_reply.started":"2024-08-29T20:07:25.043721Z","shell.execute_reply":"2024-08-29T20:07:44.496358Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"8XGyoP3A9fek"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nprint(\"TensorFlow version:\", tf.__version__)\nprint(\"TensorFlow Probability version:\", tfp.__version__)","metadata":{"id":"mgfxpLNkp--q","outputId":"f1f47a4f-6e14-45bc-8951-7bd95ada3fc1","execution":{"iopub.status.busy":"2024-08-29T20:07:44.499627Z","iopub.execute_input":"2024-08-29T20:07:44.500497Z","iopub.status.idle":"2024-08-29T20:07:44.506973Z","shell.execute_reply.started":"2024-08-29T20:07:44.500442Z","shell.execute_reply":"2024-08-29T20:07:44.505679Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"TensorFlow version: 2.16.1\nTensorFlow Probability version: 0.24.0\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip uninstall scikit-learn\n!pip install scikit-learn==1.2.2","metadata":{"id":"3-_bQegTpMTG","outputId":"c2a5f5a3-1a1c-45a4-eb7d-26e3681b981e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{"id":"YC0m-5IQvj_z"}},{"cell_type":"code","source":"uploaded = files.upload()","metadata":{"id":"0GuxyVUrvkXs","outputId":"f17a8c54-0fc2-4579-f250-510e025d9b31","execution":{"iopub.status.busy":"2024-08-26T13:32:48.250748Z","iopub.execute_input":"2024-08-26T13:32:48.251227Z","iopub.status.idle":"2024-08-26T13:32:48.287765Z","shell.execute_reply.started":"2024-08-26T13:32:48.251185Z","shell.execute_reply":"2024-08-26T13:32:48.285681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Choose the filename of the uploaded file\nfilename = '/kaggle/input/randata/OCT26_2022_M2_bLog0CFA-1deceleration_profile.txt'\n# Load the text file containing depth vs. deceleration values\ndata = np.loadtxt(filename)\ndepth = data[:, 0]\ndeceleration = data[:, 1]","metadata":{"id":"DTg46UHGvtUp","execution":{"iopub.status.busy":"2024-08-29T20:08:19.125319Z","iopub.execute_input":"2024-08-29T20:08:19.125787Z","iopub.status.idle":"2024-08-29T20:08:19.133174Z","shell.execute_reply.started":"2024-08-29T20:08:19.125745Z","shell.execute_reply":"2024-08-29T20:08:19.131598Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Create an interpolation function\ninterpolation_function = interp1d(depth, deceleration, kind='nearest')\n\n# Define the depth intervals at 0.01 m increments\ndepth_interpolated = np.arange(min(depth), max(depth), 0.01)\n\n# Use the interpolation function to get deceleration values at these intervals\ndeceleration_interpolated = interpolation_function(depth_interpolated)\n\n","metadata":{"id":"W0FY99Nevyi-","execution":{"iopub.status.busy":"2024-08-29T20:08:24.756603Z","iopub.execute_input":"2024-08-29T20:08:24.757454Z","iopub.status.idle":"2024-08-29T20:08:24.765088Z","shell.execute_reply.started":"2024-08-29T20:08:24.757403Z","shell.execute_reply":"2024-08-29T20:08:24.76383Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"\n# Define the desired shape (211,)\ndesired_shape = (211,)\n\n# Calculate the number of zeros needed to pad\nzeros_to_add = desired_shape[0] - deceleration_interpolated.shape[0]\n\n# Pad the original array with zeros to achieve the desired shape\npadded_array = np.pad(deceleration_interpolated, (0, zeros_to_add), 'constant')\npadded_array=padded_array.reshape(1,211)","metadata":{"id":"Ap2bXT0pwRwL","execution":{"iopub.status.busy":"2024-08-29T20:08:40.642232Z","iopub.execute_input":"2024-08-29T20:08:40.642704Z","iopub.status.idle":"2024-08-29T20:08:40.648943Z","shell.execute_reply.started":"2024-08-29T20:08:40.642663Z","shell.execute_reply":"2024-08-29T20:08:40.647673Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"\n# Load the saved model\nloaded_rf_model = joblib.load('/kaggle/input/randomfor/scikitlearn/default/1/random_forest_model.joblib')\n\n# Now 'loaded_rf_model' contains the Random Forest model, and you can use it for predictions.\nmax_dec = np.max(padded_array, axis=1)\npd = np.count_nonzero(padded_array, axis=1)\npredRF=np.column_stack((max_dec,pd))\nclass_probs = loaded_rf_model.predict_proba(predRF)","metadata":{"id":"T5svUDpowUj0","execution":{"iopub.status.busy":"2024-08-29T20:08:50.865111Z","iopub.execute_input":"2024-08-29T20:08:50.866028Z","iopub.status.idle":"2024-08-29T20:08:51.355733Z","shell.execute_reply.started":"2024-08-29T20:08:50.865944Z","shell.execute_reply":"2024-08-29T20:08:51.354577Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"import sklearn\nimport joblib\n\nprint('scikit-learn version:', sklearn.__version__)\nprint('joblib version:', joblib.__version__)\n","metadata":{"id":"DDEecoDBAWB_","outputId":"a0f99542-cacb-46ba-bfbe-2610eb02ef92"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_probs","metadata":{"execution":{"iopub.status.busy":"2024-08-29T20:08:57.00762Z","iopub.execute_input":"2024-08-29T20:08:57.008081Z","iopub.status.idle":"2024-08-29T20:08:57.017246Z","shell.execute_reply.started":"2024-08-29T20:08:57.008037Z","shell.execute_reply":"2024-08-29T20:08:57.015728Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"array([[0.95, 0.05, 0.  , 0.  ]])"},"metadata":{}}]},{"cell_type":"code","source":"# Original array with 4 values that sum up to 1\noriginal_array = np.array(class_probs)\n\n# Calculate the sum of the original array\noriginal_sum = np.sum(original_array)\n\n# Scale the array to sum up to 0.7\ndesired_sum = 0.6\npad=0.1\nscaled_array = (original_array / original_sum) * desired_sum\nprior=scaled_array+pad\nprint(prior)\nclasses = ['Class 1','Class 2','Class 3','Class 4']\n# plt.bar(classes,np.squeeze(prior))","metadata":{"id":"JPSgD3-VwmvB","outputId":"e153afea-37d3-4462-d39a-6c6e1f73f5af","execution":{"iopub.status.busy":"2024-08-29T20:09:04.22346Z","iopub.execute_input":"2024-08-29T20:09:04.223937Z","iopub.status.idle":"2024-08-29T20:09:04.231696Z","shell.execute_reply.started":"2024-08-29T20:09:04.223894Z","shell.execute_reply":"2024-08-29T20:09:04.230538Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"[[0.67 0.13 0.1  0.1 ]]\n","output_type":"stream"}]},{"cell_type":"code","source":"# Load the saved model\n\n# from tensorflow.keras.models import load_model\n# loaded_model = tf.saved_model.load('/content/drive/MyDrive/Journal Paper 1/main_folder')\n\nscaler = joblib.load('/kaggle/input/pffp_bnn/tensorflow2/default/1/scaler_fit.joblib')\npadded_array=padded_array*100/5.61\npred_in_scaled = scaler.transform(padded_array.reshape(-1, 1)).reshape(padded_array.shape)\npred_in_scaled = np.reshape(pred_in_scaled,[pred_in_scaled.shape[0],pred_in_scaled.shape[1],1])\n# likelihood=loaded_model.predict(pred_in_scaled)\n","metadata":{"id":"hoRQn8kl4IgD","outputId":"bf343771-a7c9-4ce3-d4e5-a27152b47152","execution":{"iopub.status.busy":"2024-08-29T20:14:27.963503Z","iopub.execute_input":"2024-08-29T20:14:27.964418Z","iopub.status.idle":"2024-08-29T20:14:27.985771Z","shell.execute_reply.started":"2024-08-29T20:14:27.96437Z","shell.execute_reply":"2024-08-29T20:14:27.984525Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator MinMaxScaler from version 1.5.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\nhttps://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow_probability as tfp\n\n# Define the kernel divergence function\ndef kernel_divergence_fn(q, p, _):\n    return tfp.distributions.kl_divergence(q, p) / 909.0\n\n# Model building\nclass BayesianConv1DModel(tf.Module):\n    def __init__(self, name=None):\n        super().__init__(name=name)\n        self.conv1 = tfp.layers.Convolution1DFlipout(32, kernel_size=3, activation='relu', \n                                                     kernel_divergence_fn=kernel_divergence_fn, \n                                                     padding='VALID')\n        self.conv2 = tfp.layers.Convolution1DFlipout(64, kernel_size=3, activation='relu', \n                                                     kernel_divergence_fn=kernel_divergence_fn, \n                                                     padding='VALID')\n        self.flatten = tf.keras.layers.Flatten()\n        self.dense1 = tfp.layers.DenseFlipout(1024, activation='relu', kernel_divergence_fn=kernel_divergence_fn)\n        self.dense2 = tfp.layers.DenseFlipout(64, activation='relu', kernel_divergence_fn=kernel_divergence_fn)\n        self.dense3 = tfp.layers.DenseFlipout(4, activation='softmax', kernel_divergence_fn=kernel_divergence_fn)\n\n    def __call__(self, x):\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.flatten(x)\n        x = self.dense1(x)\n        x = self.dense2(x)\n        x = self.dense3(x)\n        return x\n\n# Instantiate the model\nmodel = BayesianConv1DModel()\n\n# Define loss function\ndef loss_fn(labels, predictions):\n    return tf.reduce_mean(tf.keras.losses.categorical_crossentropy(labels, predictions))\n\n# Define the training step\ndef train_step(model, inputs, labels, optimizer):\n    with tf.GradientTape() as tape:\n        predictions = model(inputs)\n        loss = loss_fn(labels, predictions)\n    gradients = tape.gradient(loss, model.trainable_variables)\n    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n    return loss\n\n# # Example training loop\n# optimizer = tf.keras.optimizers.Adam()\n# for epoch in range(epochs):\n#     for inputs, labels in dataset:\n#         loss = train_step(model, inputs, labels, optimizer)\n#     print(f\"Epoch {epoch}, Loss: {loss.numpy()}\")","metadata":{"execution":{"iopub.status.busy":"2024-08-29T20:14:36.204994Z","iopub.execute_input":"2024-08-29T20:14:36.205441Z","iopub.status.idle":"2024-08-29T20:14:36.225419Z","shell.execute_reply.started":"2024-08-29T20:14:36.205403Z","shell.execute_reply":"2024-08-29T20:14:36.224061Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# Instantiate the model and optimizer\nmodel3 = BayesianConv1DModel()\noptimizer3 = tf.keras.optimizers.Adam()\n\n# Set up the checkpointing\ncheckpoint = tf.train.Checkpoint(optimizer=optimizer3, model=model3)\nmanager = tf.train.CheckpointManager(checkpoint, directory='/kaggle/input/pffp_bnn/tensorflow2/default/1/checkpoints', max_to_keep=3)\n\n# Check if there's a checkpoint available\nif manager.latest_checkpoint:\n    checkpoint.restore(manager.latest_checkpoint)\n    print(\"Model restored from checkpoint at {}\".format(manager.latest_checkpoint))\nelse:\n    print(\"No checkpoint found.\")","metadata":{"execution":{"iopub.status.busy":"2024-08-29T20:14:42.385843Z","iopub.execute_input":"2024-08-29T20:14:42.386291Z","iopub.status.idle":"2024-08-29T20:14:42.418689Z","shell.execute_reply.started":"2024-08-29T20:14:42.386249Z","shell.execute_reply":"2024-08-29T20:14:42.417382Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"Model restored from checkpoint at /kaggle/input/pffp_bnn/tensorflow2/default/1/checkpoints/ckpt-1\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\n\n# Function to predict new data\ndef predict(model, new_data):\n    \"\"\"\n    Generates predictions from the model for the given new data.\n\n    Args:\n    model: The trained Bayesian neural network model.\n    new_data: A NumPy array or a TensorFlow tensor of new data to predict.\n\n    Returns:\n    A NumPy array of predictions.\n    \"\"\"\n    # Ensure the input is a TensorFlow tensor\n    new_data = tf.convert_to_tensor(new_data, dtype=tf.float32)\n\n    # Perform a forward pass and obtain the logits\n    logits = model(new_data)\n\n    # Since the last layer uses softmax, the output will be the probabilities\n    probabilities = tf.nn.softmax(logits)\n\n    # Optionally, you can obtain the class predictions directly\n#     class_predictions = tf.argmax(logits, axis=1)\n\n    return probabilities.numpy()  # Convert probabilities to NumPy array for easier handling\n\n# Example usage\n# Assuming `X_new` is your new dataset ready to be predicted\n# X_new = np.random.random((10, sequence_length, channels))  # Dummy data for illustration\n# predictions = predict(model, X_new)\n# print(predictions)","metadata":{"id":"EZP-H92TYBns","execution":{"iopub.status.busy":"2024-08-29T20:14:46.996854Z","iopub.execute_input":"2024-08-29T20:14:46.997285Z","iopub.status.idle":"2024-08-29T20:14:47.00435Z","shell.execute_reply.started":"2024-08-29T20:14:46.997247Z","shell.execute_reply":"2024-08-29T20:14:47.003021Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"model23","metadata":{"execution":{"iopub.status.busy":"2024-08-26T13:51:47.355562Z","iopub.execute_input":"2024-08-26T13:51:47.35607Z","iopub.status.idle":"2024-08-26T13:51:47.365497Z","shell.execute_reply.started":"2024-08-26T13:51:47.356027Z","shell.execute_reply":"2024-08-26T13:51:47.364049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Example usage\n# Assuming `X_new` is your new dataset ready to be predicted\n# X_new = np.random.random((10, sequence_length, channels))  # Dummy data for illustration\npredictions = predict(model3, pred_in_scaled)\nclass_predictions = tf.argmax(predictions, axis=1)\nprint(predictions)\nprint(class_predictions)","metadata":{"id":"a3Yl9dG7YDKu","outputId":"f9fe267d-c9ed-48c5-a303-d76e76847c33","execution":{"iopub.status.busy":"2024-08-29T20:14:55.232549Z","iopub.execute_input":"2024-08-29T20:14:55.232965Z","iopub.status.idle":"2024-08-29T20:14:56.272646Z","shell.execute_reply.started":"2024-08-29T20:14:55.232927Z","shell.execute_reply":"2024-08-29T20:14:56.271394Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/tensorflow_probability/python/layers/util.py:99: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use the `layer.add_weight()` method instead.\n  loc = add_variable_fn(\n/opt/conda/lib/python3.10/site-packages/tensorflow_probability/python/layers/util.py:109: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use the `layer.add_weight()` method instead.\n  untransformed_scale = add_variable_fn(\n","output_type":"stream"},{"name":"stdout","text":"[[0.47536692 0.17487772 0.17487772 0.17487772]]\ntf.Tensor([0], shape=(1,), dtype=int64)\n","output_type":"stream"}]},{"cell_type":"code","source":"from tensorflow.keras.models import load_model\n\n# Load the model\nmodel = tf.saved_model.load('/content/drive/MyDrive/Journal Paper 1/main_folder')\n","metadata":{"id":"FWKu6AS6AQG0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions*prior","metadata":{"id":"VBoQY2wZW17_","execution":{"iopub.status.busy":"2024-08-26T14:01:58.424412Z","iopub.execute_input":"2024-08-26T14:01:58.424969Z","iopub.status.idle":"2024-08-26T14:01:58.435857Z","shell.execute_reply.started":"2024-08-26T14:01:58.42492Z","shell.execute_reply":"2024-08-26T14:01:58.433825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ss =[]\n# n= int(input())\n\n\nc = [1,2,3,4]\nplt.figure()\nfig, ax = plt.subplots(1,2)\n\nfor i in range(0,30):\n  s = loaded_model.predict(pred_in_scaled,verbose = 1)\n  post=((prior)*s)\n  final_s = post/np.sum(post)\n  ss.append(final_s)\n  ax[1].scatter(c,final_s,c=\"blue\",alpha=0.1)\n# ss=np.array(ss)\n# sss = pd.DataFrame(np.squeeze(ss))\nsss = np.squeeze(ss)\nax[1].boxplot(sss)\nax[1].set_xticklabels(classes)\n# ax[1].text(1.8, -0.25, 'Actual Result: Class '+str(y_train[n]+1), color='red')\n\n\nax[0].plot(deceleration,depth*100, label=filename)\nax[0].invert_yaxis()\nax[0].set_ylabel(\"Depth [cm]\")\nax[0].set_xlabel(\"Deceleration[g]\")","metadata":{"id":"GP7apsqn4nFj","outputId":"c573dcbc-292a-4c62-d0d1-8f358ba760ae"},"execution_count":null,"outputs":[]}]}